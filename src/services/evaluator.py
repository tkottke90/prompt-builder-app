from langchain_core.prompts.chat import ChatPromptTemplate
from langchain_community.chat_models.ollama import ChatOllama
from langchain.output_parsers import ResponseSchema, StructuredOutputParser
from pydantic import BaseModel, Field
from src.models.prompt_model import PromptDTO
from typing import Any
import json
import logging
import re

PROMPT_INPUT_REGEX = re.compile('(\{([^!:}]*)(?:[:!]?([^}]*))?\})', re.DOTALL)

EvaluatorLogger = logging.getLogger('Evaluator')

testllm = ChatOllama(
  model="mistral:7b",
  temperature=0.1,
  format="json"
)

class PromptArgument(BaseModel):
  label: str = Field()
  formatting: str = Field()

def scoreGeneration(prompt: str, generation: str):
  template = ChatPromptTemplate.from_template(
    partial_variables={
      "format_instructions": StructuredOutputParser.from_response_schemas([
        ResponseSchema(name="score", description="a grading between 0 and 10 of how well the template performed", type="float"),
        ResponseSchema(name="suggestions", description="a list of 5 adjustments to the template which could improve outcomes", type="list[str]"),
        ResponseSchema(name="questions", description="a list of questions for the user about the template", type="list[str]")
      ])
    },
    template=
"""# Instructions
You are an AI Assistant specializing in Generative AI using Natural Language.
You task is to evaluate the following LLM Prompt Template and LLM Response and provide feedback to the template author to assist them in generating an effective prompt for LLM driven applications.
Use the step by step program provided below to evaluate Prompt Template and Generated Response.
"Prompt Template" was the template provided to the LLM by the user
"Generated Response" was a response generated by an LLM when provided with the template
You should respond using the following format:

{format_instructions}

## Prompt Template
{template}

## Generated Response
{generation}

# Program
1. generate a grading between 0 and 10 of how well the Generated Response responded to the Prompt Template
2. generate 5 suggested adjustments to try which would improve the outcome
3. generate 5 questions about the Prompt Template that would help you generate suggestions
""")
  
  chain = template | testllm

  result = chain.invoke(input={ "template": prompt, "generation": generation })
  data = json.loads(result.content)

  return {
    "score": data.get('score', -1),
    "suggestions": data.get('suggestions', []),
    "questions": data.get('questions', [])
  }

def testPrompt(prompt: PromptDTO, args: list[Any], kwargs: dict[str, Any]):
  """
  Call LLM using a provided prompt and return the results of that prompt
  """
  EvaluatorLogger.info(msg=f'Starting test for prompt: {prompt.id}', extra={ "inputs": kwargs })

  template = ChatPromptTemplate.from_template(template=prompt.value)
  chain = template | testllm

  result = chain.invoke(input=kwargs)

  EvaluatorLogger.info(msg=f'Test complete', extra={ "run_id": result.id, "response": result })

  return (result.content, template.format(**kwargs))


def getPromptInputs(prompt: PromptDTO):
  """
    Extracts the placeholders in a prompt.  Returning labeled placeholders as keyword args and unlabeled as positional args
  """
  positionalArgs: list[PromptArgument] = []
  keywordArgs: dict[str, PromptArgument] = {}

  for (match, key, formatting) in re.findall(PROMPT_INPUT_REGEX, prompt.value):
    arg = PromptArgument(label=match, formatting=formatting)

    if (len(key) == 0):
      positionalArgs.append(arg)
    else:
      keywordArgs.update({ key: arg })


  return {
    "positional": positionalArgs,
    "keyword": keywordArgs
  }